# ğŸ§Š Homework Week 1 â€” Apache Iceberg & PyIceberg

> This assignment involves setting up a daily-partitioned Iceberg table using **PyIceberg**, fetching MAANG stock prices from **Polygon API**, and writing/merging data via **AWS Glue jobs**. Bonus: fast-forward a branch to main.

---

## âœ… Requirements Recap

- Use **PyIceberg** (or **PySpark**), **Polygon API**, and **Tabular**.
- Create a **daily partitioned table** in your schema.
- Make an **audit branch** with `create_branch()`.
- Load daily stock data and write into the audit branch.
- Save logic in `stock_prices.py` and upload zipped file to DataExpert.
- **Bonus**: Write a job that fast-forwards the audit branch to main.

---

## ğŸ§¾ Files

### 1. `stock_prices.py`

This file:
- Initializes Iceberg schema and table
- Sets up branching and fast-forwarding logic
- Wraps Glue job orchestration

**Highlights:**
- Uses `pyiceberg` to define schema, partition, and sort
- Leverages AWS Glue via helper `create_and_run_glue_job`
- Supports conditional execution for writing or merging branches

```python
# Functions: create_table_if_not_exists, write_to_branch(date), fast_forwarding(date)
# Uses catalog.create_table_if_not_exists() and Glue job execution
```

### 2. `stock_prices_branching.py`

This Glue job:
- Fetches MAANG stock prices from **Polygon.io** using `requests`
- Converts result into a PySpark DataFrame
- Writes to Iceberg table partitioned by `date`

**Highlights:**
- Handles structured transformation and schema mapping
- Uses `ARRAY_AGG`, `OBJECT_CONSTRUCT`, and PySpark `.writeTo()`
- Supports writing to branches (optional `--branch` arg)
- Contains cleanup logic for audit branches if unused

```python
# Functions: fetch_stock_data(), main()
# Key tools: requests, pyspark.sql.functions, AWS GlueContext
```

---

## ğŸ§ª Features Implemented

| Feature                                      | Status     | Notes                                                            |
|---------------------------------------------|------------|------------------------------------------------------------------|
| Daily-partitioned Iceberg table             | âœ… Done     | Uses `PartitionSpec` on `date`                                   |
| Branching (WAP audit)                       | âœ… Done     | Implemented via Glue job and CLI args                            |
| Stock data from Polygon API                 | âœ… Done     | Validated against real API responses                             |
| Upload script to DataExpert                 | âœ… Done     | `stock_prices.py` zipped and uploaded                           |
| Fast-forward logic                          | âœ… Done     | Job handles fast-forward from `audit_branch` to `main`           |

---

## ğŸ§  Feedback Summary

### Strengths
- **âœ… Well-defined Iceberg schema and partitions**
- **âœ… Correct API usage and data ingestion**
- **âœ… Glue orchestration and branching logic implemented**

### Suggestions
- ğŸ”’ **Secure API key management** â†’ Use env variables or Vault
- ğŸ” **Better modularization** â†’ Use CLI flags vs manual uncommenting
- âš™ï¸ **Add more error handling** â†’ Especially around Glue and API I/O
- ğŸ§¹ **Smarter branch cleanup** â†’ Avoid accidental removal of prod branches
- âš¡ **Improve performance** â†’ Consider parallelism when loading stock data

---

## ğŸ Final Assessment

```json
{
  "letter_grade": "A",
  "passes": true
}
```

You have delivered a complete and well-engineered solution. Just a few refinements around security, code structure and resilience would elevate this even further. Great job!

---

ğŸ“ **Next Step:**
- Incorporate feedback into future submissions
- Consider parameterizing the pipeline (e.g., CLI args or config files)
- Keep exploring advanced Iceberg features like snapshot expiration and schema evolution
